{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# categories: 18 ['Czech', 'German', 'Arabic', 'Japanese', 'Chinese', 'Vietnamese', 'Russian', 'French', 'Irish', 'English', 'Spanish', 'Greek', 'Italian', 'Portuguese', 'Scottish', 'Dutch', 'Korean', 'Polish']\n",
      "O'Neal\n"
     ]
    }
   ],
   "source": [
    "# this is copy pasted from tutorial\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "n_letters = len(all_letters) + 1 # Plus EOS marker\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    with open(filename, encoding='utf-8') as some_file:\n",
    "        return [unicodeToAscii(line.strip()) for line in some_file]\n",
    "\n",
    "# Build the category_lines dictionary, a list of lines per category\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "for filename in findFiles('../char-rnn-classify/data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "if n_categories == 0:\n",
    "    raise RuntimeError('Data not found. Make sure that you downloaded data '\n",
    "        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n",
    "        'the current directory.')\n",
    "\n",
    "print('# categories:', n_categories, all_categories)\n",
    "print(unicodeToAscii(\"O'Néàl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_categories):\n",
    "        super(Model,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
    "\n",
    "        self.o2o = nn.Linear(output_size + hidden_size, output_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    \n",
    "    def forward(self, category, input, hidden):        \n",
    "        combined = torch.cat((category, input, hidden), 1)\n",
    "\n",
    "        out = self.i2o(combined)\n",
    "        hidden = self.i2h(combined)\n",
    "\n",
    "        combined = torch.cat((out, hidden), 1)\n",
    "\n",
    "        out = self.o2o(combined)\n",
    "        out = self.dropout(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "\n",
    "        return (out, hidden)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset\n",
    "\n",
    "1. For each word hot encode it, add 1 special EOS char in the end.\n",
    "2. Split each word into first n-1 and last n-1 chars.\n",
    "3. Each batch will be just one word. Feed consecutively each letter and accumulate loss over the word and then call backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getCategoryIndex(category, all_categories):\n",
    "    return all_categories.index(category)\n",
    "\n",
    "def encodeCategory(category, all_categories, all_categories_size):\n",
    "    index = getCategoryIndex(category, all_categories)\n",
    "    tensor = torch.zeros(1, all_categories_size)\n",
    "    tensor[0,index] = 1.0\n",
    "\n",
    "    return tensor\n",
    "\n",
    "getCategoryIndex('English',all_categories)\n",
    "\n",
    "encodeCategory('English',all_categories, len(all_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels all_categories \n",
    "# dict category_lines\n",
    "\n",
    "# LongTensor of second letter to end (EOS) for target\n",
    "def targetTensor(line):\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n",
    "# each tuple in dataset = (direction vec + encoded letter)\n",
    "dataset = []\n",
    "\n",
    "all_categories_size = len(all_categories)\n",
    "\n",
    "for category in all_categories:\n",
    "    # print(category)\n",
    "    category_encoded = encodeCategory(category, all_categories, all_categories_size)\n",
    "    for word in category_lines[category]:\n",
    "        word_begin = word\n",
    "        word_end = word\n",
    "        word_begin_tensor = lineToTensor(word_begin)\n",
    "        word_end_tensor = targetTensor(word_end)\n",
    "\n",
    "        dataset.append((category_encoded, word_begin_tensor, word_end_tensor))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20074"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(dataset)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7],\n",
       "        [ 4],\n",
       "        [11],\n",
       "        [58]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][2].unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(n_letters, 128, n_letters, len(all_categories))\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch......  0  loss......  tensor(20.6776, grad_fn=<AddBackward0>)\n",
      "epoch......  1  loss......  tensor(16.5187, grad_fn=<AddBackward0>)\n",
      "epoch......  2  loss......  tensor(15.4509, grad_fn=<AddBackward0>)\n",
      "epoch......  3  loss......  tensor(13.8330, grad_fn=<AddBackward0>)\n",
      "epoch......  4  loss......  tensor(18.1565, grad_fn=<AddBackward0>)\n",
      "epoch......  5  loss......  tensor(17.7501, grad_fn=<AddBackward0>)\n",
      "epoch......  6  loss......  tensor(16.8465, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/roman/workspace/dl-experiments/char-rnn-generate/generate.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn-generate/generate.ipynb#ch0000009?line=10'>11</a>\u001b[0m hidden \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minit_hidden()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn-generate/generate.ipynb#ch0000009?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m letter_index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(letter_input\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn-generate/generate.ipynb#ch0000009?line=12'>13</a>\u001b[0m     output, hidden \u001b[39m=\u001b[39m model(direction, letter_input[letter_index], hidden)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn-generate/generate.ipynb#ch0000009?line=13'>14</a>\u001b[0m     \u001b[39m# print(output, letter_target)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn-generate/generate.ipynb#ch0000009?line=14'>15</a>\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m criterion(output, target[letter_index])\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/roman/workspace/dl-experiments/char-rnn-generate/generate.ipynb Cell 11\u001b[0m in \u001b[0;36mModel.forward\u001b[0;34m(self, category, input, hidden)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn-generate/generate.ipynb#ch0000009?line=14'>15</a>\u001b[0m combined \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((category, \u001b[39minput\u001b[39m, hidden), \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn-generate/generate.ipynb#ch0000009?line=16'>17</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mi2o(combined)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn-generate/generate.ipynb#ch0000009?line=17'>18</a>\u001b[0m hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mi2h(combined)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn-generate/generate.ipynb#ch0000009?line=19'>20</a>\u001b[0m combined \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((out, hidden), \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn-generate/generate.ipynb#ch0000009?line=21'>22</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mo2o(combined)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for data in dataset:\n",
    "        direction, letter_input, letter_target = data\n",
    "        \n",
    "        target = letter_target.unsqueeze(-1)\n",
    "        model.zero_grad()\n",
    "        loss = torch.tensor(0.0)\n",
    "        hidden = model.init_hidden()\n",
    "        for letter_index in range(letter_input.size(0)):\n",
    "            output, hidden = model(direction, letter_input[letter_index], hidden)\n",
    "            # print(output, letter_target)\n",
    "            loss += criterion(output, target[letter_index])\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "    print('epoch...... ', epoch, ' loss...... ', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL out dropout tensor([[ 0.0574,  0.0211,  0.0228, -0.0312, -0.0062, -0.1348,  0.0022,  0.0489,\n",
      "         -0.0158, -0.0484, -0.0719,  0.0843, -0.0000,  0.1007, -0.1185,  0.0501,\n",
      "         -0.0481, -0.1293, -0.0082, -0.0264, -0.0244,  0.0880,  0.0227, -0.0000,\n",
      "         -0.1021,  0.0621,  0.0880, -0.0335,  0.0808,  0.0581, -0.0517, -0.0532,\n",
      "         -0.0482, -0.0418, -0.0252,  0.0937,  0.0238, -0.0197,  0.0867, -0.0613,\n",
      "         -0.0557, -0.0036, -0.0829, -0.0090,  0.0684, -0.0000,  0.0000, -0.0107,\n",
      "         -0.0363,  0.0151, -0.0607, -0.0394,  0.0309,  0.0516,  0.0468,  0.1260,\n",
      "         -0.0000, -0.0405,  0.0000]], grad_fn=<MulBackward0>)\n",
      "MODEL out softmax tensor([[-4.0200, -4.0564, -4.0546, -4.1086, -4.0837, -4.2123, -4.0753, -4.0285,\n",
      "         -4.0932, -4.1259, -4.1493, -3.9931, -4.0774, -3.9767, -4.1959, -4.0274,\n",
      "         -4.1255, -4.2068, -4.0856, -4.1039, -4.1018, -3.9894, -4.0548, -4.0774,\n",
      "         -4.1795, -4.0153, -3.9895, -4.1109, -3.9967, -4.0193, -4.1292, -4.1306,\n",
      "         -4.1257, -4.1192, -4.1027, -3.9837, -4.0537, -4.0971, -3.9907, -4.1387,\n",
      "         -4.1332, -4.0810, -4.1603, -4.0865, -4.0091, -4.0774, -4.0774, -4.0881,\n",
      "         -4.1137, -4.0623, -4.1381, -4.1168, -4.0465, -4.0258, -4.0307, -3.9514,\n",
      "         -4.0774, -4.1179, -4.0774]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-4.0200, -4.0564, -4.0546, -4.1086, -4.0837, -4.2123, -4.0753, -4.0285,\n",
       "         -4.0932, -4.1259, -4.1493, -3.9931, -4.0774, -3.9767, -4.1959, -4.0274,\n",
       "         -4.1255, -4.2068, -4.0856, -4.1039, -4.1018, -3.9894, -4.0548, -4.0774,\n",
       "         -4.1795, -4.0153, -3.9895, -4.1109, -3.9967, -4.0193, -4.1292, -4.1306,\n",
       "         -4.1257, -4.1192, -4.1027, -3.9837, -4.0537, -4.0971, -3.9907, -4.1387,\n",
       "         -4.1332, -4.0810, -4.1603, -4.0865, -4.0091, -4.0774, -4.0774, -4.0881,\n",
       "         -4.1137, -4.0623, -4.1381, -4.1168, -4.0465, -4.0258, -4.0307, -3.9514,\n",
       "         -4.0774, -4.1179, -4.0774], grad_fn=<SelectBackward0>),\n",
       " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "direction, letter_input, letter_target = dataset[5]\n",
    "letter_target.unsqueeze(-1)\n",
    "\n",
    "model.zero_grad()\n",
    "loss = torch.tensor(0.0)\n",
    "hidden = model.init_hidden()\n",
    "output, hidden = model(direction, letter_input[letter_index], hidden)\n",
    "\n",
    "output[0], letter_target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.0890, -4.1476, -4.0393, -4.0800, -4.1578, -4.2367, -4.0268, -4.1474,\n",
      "        -4.0063, -4.0855, -4.1446, -4.0063, -4.0875, -4.0875, -4.1381, -4.0075,\n",
      "        -4.0765, -4.1169, -4.1314, -4.0061, -4.0875, -3.9981, -3.9554, -4.1450,\n",
      "        -4.0539, -4.0909, -4.1167, -4.0501, -4.0870, -4.0906, -4.0742, -4.1271,\n",
      "        -4.0967, -4.0601, -4.0875, -3.9578, -4.2020, -4.0369, -4.0052, -4.0136,\n",
      "        -4.1084, -4.0875, -4.0747, -4.1100, -4.1057, -3.9904, -4.1494, -4.0889,\n",
      "        -4.0875, -4.1546, -4.0208, -3.9853, -4.1667, -4.0137, -4.0875, -4.0777,\n",
      "        -4.0552, -4.0573, -4.0979])\n",
      "tensor(22)\n",
      "tensor(-3.9554)\n"
     ]
    }
   ],
   "source": [
    "t = [-0.0015, -0.0601,  0.0482,  0.0075, -0.0703, -0.1492,  0.0607, -0.0599,\n",
    "          0.0812,  0.0020, -0.0571,  0.0812,  0.0000, -0.0000, -0.0506,  0.0800,\n",
    "          0.0110, -0.0294, -0.0439,  0.0814, -0.0000,  0.0894,  0.1321, -0.0575,\n",
    "          0.0336, -0.0034, -0.0292,  0.0374,  0.0005, -0.0031,  0.0133, -0.0396,\n",
    "         -0.0092,  0.0274,  0.0000,  0.1297, -0.1145,  0.0506,  0.0823,  0.0739,\n",
    "         -0.0209, -0.0000,  0.0128, -0.0225, -0.0182,  0.0971, -0.0619, -0.0014,\n",
    "          0.0000, -0.0671,  0.0667,  0.1022, -0.0792,  0.0738, -0.0000,  0.0098,\n",
    "          0.0323,  0.0302, -0.0104]\n",
    "t = torch.tensor(t)\n",
    "\n",
    "a = F.log_softmax(t.float(), dim=0)\n",
    "print(a)\n",
    "print(a.argmax())\n",
    "print(a[a.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just return an output given a line\n",
    "def evaluate(direction, first_letter):\n",
    "    with torch.no_grad():\n",
    "        direction_tensor = encodeCategory(direction,all_categories,all_categories_size)\n",
    "        letter_tensor = letterToTensor(first_letter)\n",
    "        hidden = model.init_hidden()\n",
    "\n",
    "        limit = 10\n",
    "\n",
    "        outputs = \"\"\n",
    "        for i in range(limit):\n",
    "            output, hidden = model(direction_tensor, letter_tensor, hidden)\n",
    "            position = torch.argmax(output)\n",
    "            if position == len(all_letters):\n",
    "                break\n",
    "            outputs += all_letters[position]\n",
    "            letter_tensor = output\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kk'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('Japanese','O')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
