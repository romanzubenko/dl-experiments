{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/gabrielloye/RNN-walkthrough/blob/master/main.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./drake_archive/drake_lyrics.txt') as f:\n",
    "    text_dataset_raw = f.read()\n",
    "\n",
    "text_dataset_raw = text_dataset_raw.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakdown dataset into sequences of SENTENCE_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wassup to all the ladies on the northside, southside, eastside, ',\n",
       " \"she moved out of state, and shit done went left, she's seekin' f\",\n",
       " \"i watch her climb to the top of the pole and then get to slidin'\",\n",
       " \"i'ma just give it to you direct, instead of me throwin' this shi\",\n",
       " 'we used to do pornos when you would come over but now you got mo']"
      ]
     },
     "execution_count": 813,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SENTENCE_LEN = 64\n",
    "text_dataset = re.findall('.'*SENTENCE_LEN,text_dataset_raw)\n",
    "\n",
    "text_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text_dataset\n",
    "\n",
    "# Join all the sentences together and extract the unique characters from the combined sentences\n",
    "chars = set(''.join(text))\n",
    "\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad all inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wassup to all the ladies on the northside, southside, eastside, ',\n",
       " \"she moved out of state, and shit done went left, she's seekin' f\",\n",
       " \"i watch her climb to the top of the pole and then get to slidin'\",\n",
       " \"i'ma just give it to you direct, instead of me throwin' this shi\",\n",
       " 'we used to do pornos when you would come over but now you got mo']"
      ]
     },
     "execution_count": 818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = len(max(text, key=len))\n",
    "\n",
    "for i in range(len(text)):\n",
    "    while len(text[i]) < maxlen:\n",
    "        text[i] += ' '\n",
    "\n",
    "text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wassup to all the ladies on the northside, southside, eastside,',\n",
       "  'assup to all the ladies on the northside, southside, eastside, '),\n",
       " (\"she moved out of state, and shit done went left, she's seekin' \",\n",
       "  \"he moved out of state, and shit done went left, she's seekin' f\"),\n",
       " ('i watch her climb to the top of the pole and then get to slidin',\n",
       "  \" watch her climb to the top of the pole and then get to slidin'\"),\n",
       " (\"i'ma just give it to you direct, instead of me throwin' this sh\",\n",
       "  \"'ma just give it to you direct, instead of me throwin' this shi\"),\n",
       " ('we used to do pornos when you would come over but now you got m',\n",
       "  'e used to do pornos when you would come over but now you got mo')]"
      ]
     },
     "execution_count": 842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_set = []\n",
    "\n",
    "for sentence in text:\n",
    "    train_data_set.append((sentence[:-1],sentence[1:]))\n",
    "\n",
    "train_data_set[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_data_set)):\n",
    "    train_data_set[i] = ([char2int[char] for char in train_data_set[i][0]], [char2int[char] for char in train_data_set[i][1]])\n",
    "\n",
    "# train_data_set[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = 8\n",
    "\n",
    "def one_hot_encode(seq, dict_size, seq_len):\n",
    "    features = np.zeros((seq_len, dict_size), dtype=np.float32)\n",
    "    for i in range(seq_len):\n",
    "        features[i,seq[i]] = 1\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hot encode all inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_encoded_train = [(one_hot_encode(train_data_set[i][0], dict_size, seq_len), one_hot_encode(train_data_set[i][1], dict_size, seq_len)) for i in range(len(train_data_set))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hot_encoded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 850,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(hot_encoded_train)\n",
    "batch_size = 8\n",
    "\n",
    "target_batched = torch.utils.data.DataLoader(hot_encoded_train[:], batch_size=batch_size, shuffle=True) \n",
    "len(target_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda:\n",
    "    device = torch.device('gpu')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout=0.1):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # out = self.dropout(out)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        out = F.softmax(out, dim=1)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=500, n_layers=3)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define hyperparameters\n",
    "\n",
    "lr=0.001\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "# criterion = nn.SmoothL1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10............. Loss: 0.0128\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "# Training Run\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    batch_index = 0\n",
    "    for data in target_batched:\n",
    "        batch_index += 1\n",
    "        x , y = data\n",
    "\n",
    "        x = torch.Tensor(x)\n",
    "        y = torch.Tensor(y)\n",
    "\n",
    "        # use .stack for not hot encoded output\n",
    "\n",
    "        input_seq = x.to(device)\n",
    "        target_seq = y.to(device)        \n",
    "\n",
    "        optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "\n",
    "        output, hidden = model(input_seq)\n",
    "        output = output.to(device)\n",
    "\n",
    "        loss = criterion(output.view(-1), target_seq.view(-1))\n",
    "        loss.backward() # Does backpropagation and calculates gradients\n",
    "        optimizer.step() # Updates the weights accordingly\n",
    "        \n",
    "    # if epoch % 10 == 0:\n",
    "    print('Epoch: {}/{}.............'.format(epoch, N_EPOCHS), end=' ')\n",
    "    print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, character):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([char2int[c] for c in character])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[0])\n",
    "    character = torch.tensor([character])\n",
    "    character = character.to(device)\n",
    "    \n",
    "    out, hidden = model(character)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    # Taking the class with the highest probability score from the output\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return int2char[char_ind], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, out_len, start='hey'):\n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    print(\"chars\", chars)\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars ['i', ' ', 'c', 'a']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"i can't to the tore, the way to the tore, the way to the tore, the way to the tore, the way to the t\""
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(model, 100, start= 'i ca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..........'"
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training notes\n",
    "\n",
    "1. nn.SmoothL1Loss() produced nice words\n",
    "    BCELoss was pretty repetetive after few words\n",
    "    MSE worked good as well\n",
    "    Crossentropy not so good\n",
    "2. Genrally 30-50 hidden dim\n",
    "3. 3-5 hidden layers\n",
    "4. 30 epochs was generally good results \n",
    "5. Batch size 8-18\n",
    "6. On smaller inputs 200 epochs was good results\n",
    "\n",
    "\n",
    "# Best Results\n",
    "SENTENCE_LEN = 64\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 8\n",
    "No Dropout\n",
    "MSELoss\n",
    "hidden_dim=500, n_layers=3\n",
    "Adam\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
