{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/gabrielloye/RNN-walkthrough/blob/master/main.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('t8.shakespeare.txt') as f:\n",
    "    text_dataset_raw = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the 100',\n",
       " 'th Etext file p',\n",
       " 'resented by Pro',\n",
       " 'ject Gutenberg,',\n",
       " 'is presented in',\n",
       " ' cooperation wi',\n",
       " 'th World Librar',\n",
       " 'y, Inc., from t',\n",
       " 'Library of the ',\n",
       " 'Future and Shak']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset = re.findall('...............',text_dataset_raw)\n",
    "\n",
    "text_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = ['hey how are you','good i am fine','have a nice day']\n",
    "text = text_dataset\n",
    "\n",
    "# Join all the sentences together and extract the unique characters from the combined sentences\n",
    "chars = set(''.join(text))\n",
    "\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the 100',\n",
       " 'th Etext file p',\n",
       " 'resented by Pro',\n",
       " 'ject Gutenberg,',\n",
       " 'is presented in',\n",
       " ' cooperation wi',\n",
       " 'th World Librar',\n",
       " 'y, Inc., from t',\n",
       " 'Library of the ',\n",
       " 'Future and Shak',\n",
       " 'espeare CDROMS.',\n",
       " '  Project Guten',\n",
       " 'often releases ',\n",
       " 'Etexts that are',\n",
       " ' NOT placed in ',\n",
       " 'the Public Doma',\n",
       " '*This Etext has',\n",
       " ' certain copyri',\n",
       " 'ght implication',\n",
       " 's you should re',\n",
       " '<<THIS ELECTRON',\n",
       " 'IC VERSION OF T',\n",
       " 'HE COMPLETE WOR',\n",
       " 'SHAKESPEARE IS ',\n",
       " 'COPYRIGHT 1990-',\n",
       " '1993 BY WORLD L',\n",
       " 'IBRARY, INC., A',\n",
       " 'PROVIDED BY PRO',\n",
       " 'JECT GUTENBERG ',\n",
       " 'ETEXT OF ILLINO',\n",
       " 'IS BENEDICTINE ',\n",
       " 'WITH PERMISSION',\n",
       " '.  ELECTRONIC A',\n",
       " 'ND MACHINE READ',\n",
       " 'ABLE COPIES MAY',\n",
       " 'DISTRIBUTED SO ',\n",
       " 'LONG AS SUCH CO',\n",
       " 'PIES (1) ARE FO',\n",
       " 'R YOUR OR OTHER',\n",
       " 'PERSONAL USE ON',\n",
       " 'LY, AND (2) ARE',\n",
       " ' NOT DISTRIBUTE',\n",
       " 'COMMERCIALLY.  ',\n",
       " 'PROHIBITED COMM',\n",
       " 'ERCIAL DISTRIBU',\n",
       " 'TION INCLUDES B',\n",
       " 'SERVICE THAT CH',\n",
       " 'ARGES FOR DOWNL',\n",
       " 'OAD TIME OR FOR',\n",
       " '*Project Gutenb',\n",
       " 'erg is proud to',\n",
       " ' cooperate with',\n",
       " ' The World Libr',\n",
       " 'in the presenta',\n",
       " 'tion of The Com',\n",
       " 'plete Works of ',\n",
       " 'William Shakesp',\n",
       " 'for your readin',\n",
       " 'g for education',\n",
       " ' and entertainm',\n",
       " 'ent.  HOWEVER, ',\n",
       " 'IS NEITHER SHAR',\n",
       " 'EWARE NOR PUBLI',\n",
       " 'C DOMAIN. . .AN',\n",
       " 'D UNDER THE LIB',\n",
       " 'OF THE FUTURE C',\n",
       " 'ONDITIONS OF TH',\n",
       " 'IS PRESENTATION',\n",
       " '. . .NO CHARGES',\n",
       " 'BE MADE FOR *AN',\n",
       " 'Y* ACCESS TO TH',\n",
       " 'IS MATERIAL.  Y',\n",
       " 'OU ARE ENCOURAG',\n",
       " 'TO GIVE IT AWAY',\n",
       " ' TO ANYONE YOU ',\n",
       " 'LIKE, BUT NO CH',\n",
       " 'ARGES ARE ALLOW',\n",
       " '**Welcome To Th',\n",
       " 'e World of Free',\n",
       " ' Plain Vanilla ',\n",
       " 'Electronic Text',\n",
       " '**Etexts Readab',\n",
       " 'le By Both Huma',\n",
       " 'ns and By Compu',\n",
       " 'ters, Since 197',\n",
       " '*These Etexts P',\n",
       " 'repared By Hund',\n",
       " 'reds of Volunte',\n",
       " 'ers and Donatio',\n",
       " 'Information on ',\n",
       " 'contacting Proj',\n",
       " 'ect Gutenberg t',\n",
       " 'o get Etexts, a',\n",
       " 'further informa',\n",
       " 'tion is include',\n",
       " 'd below.  We ne',\n",
       " 'ed your donatio',\n",
       " 'The Complete Wo',\n",
       " 'rks of William ',\n",
       " 'January, 1994  ',\n",
       " 'The Library of ',\n",
       " 'the Future Comp',\n",
       " 'lete Works of W',\n",
       " 'illiam Shakespe',\n",
       " 'Library of the ',\n",
       " 'Future is a Tra',\n",
       " 'deMark (TM) of ',\n",
       " 'World Library I',\n",
       " '******This file',\n",
       " ' should be name',\n",
       " 'd shaks12.txt o',\n",
       " 'r shaks12.zip**',\n",
       " 'Corrected EDITI',\n",
       " 'ONS of our etex',\n",
       " 'ts get a new NU',\n",
       " 'MBER, shaks13.t',\n",
       " 'VERSIONS based ',\n",
       " 'on separate sou',\n",
       " 'rces get new LE',\n",
       " 'TTER, shaks10a.',\n",
       " 'If you would li',\n",
       " 'ke further info',\n",
       " 'rmation about W',\n",
       " 'orld Library, I',\n",
       " 'Please call the',\n",
       " 'm at 1-800-443-',\n",
       " '0238 or email j',\n",
       " 'ulianc@netcom.c',\n",
       " 'Please give the',\n",
       " 'm our thanks fo',\n",
       " 'r their Shakesp',\n",
       " 'eare cooperatio',\n",
       " 'The official re',\n",
       " 'lease date of a',\n",
       " 'll Project Gute',\n",
       " 'nberg Etexts is',\n",
       " 'Midnight, Centr',\n",
       " 'al Time, of the',\n",
       " ' last day of th',\n",
       " 'e stated month.',\n",
       " 'preliminary ver',\n",
       " 'sion may often ',\n",
       " 'be posted for s',\n",
       " 'uggestion, comm',\n",
       " 'and editing by ',\n",
       " 'those who wish ',\n",
       " 'to do so.  To b',\n",
       " 'e sure you have',\n",
       " 'up to date firs',\n",
       " 't edition [xxxx',\n",
       " 'x10x.xxx] pleas',\n",
       " 'e check file si',\n",
       " 'in the first we',\n",
       " 'ek of the next ',\n",
       " 'month.  Since o',\n",
       " 'ur ftp program ',\n",
       " 'a bug in it tha',\n",
       " 't scrambles the',\n",
       " ' date [tried to',\n",
       " ' fix and failed',\n",
       " 'look at the fil',\n",
       " 'e size will hav',\n",
       " 'e to do, but we',\n",
       " ' will try to se',\n",
       " 'new copy has at',\n",
       " ' least one byte',\n",
       " 'Information abo',\n",
       " 'ut Project Gute',\n",
       " 'nberg (one page',\n",
       " 'We produce abou',\n",
       " 't two million d',\n",
       " 'ollars for each',\n",
       " ' hour we work. ',\n",
       " 'fifty hours is ',\n",
       " 'one conservativ',\n",
       " 'e estimate for ',\n",
       " 'how long it we ',\n",
       " 'to get any etex',\n",
       " 't selected, ent',\n",
       " 'ered, proofread',\n",
       " ', edited, copyr',\n",
       " 'searched and an',\n",
       " 'alyzed, the cop',\n",
       " 'yright letters ',\n",
       " 'written, etc.  ',\n",
       " 'projected audie',\n",
       " 'nce is one hund',\n",
       " 'red million rea',\n",
       " 'ders.  If our v',\n",
       " 'per text is nom',\n",
       " 'inally estimate',\n",
       " 'd at one dollar',\n",
       " ', then we produ',\n",
       " 'million dollars',\n",
       " ' per hour this ',\n",
       " 'year we, will h',\n",
       " 'ave to do four ',\n",
       " 'files per month',\n",
       " ':  thus upping ',\n",
       " 'our productivit',\n",
       " 'y from one mill',\n",
       " 'The Goal of Pro',\n",
       " 'ject Gutenberg ',\n",
       " 'is to Give Away',\n",
       " ' One Trillion E',\n",
       " 'Files by the De',\n",
       " 'cember 31, 2001',\n",
       " '.  [10,000 x 10',\n",
       " '0,000,000=Trill',\n",
       " 'This is ten tho',\n",
       " 'usand titles ea',\n",
       " 'ch to one hundr',\n",
       " 'ed million read',\n",
       " 'which is 10% of',\n",
       " ' the expected n',\n",
       " 'umber of comput',\n",
       " 'er users by the',\n",
       " 'of the year 200',\n",
       " 'We need your do',\n",
       " 'nations more th',\n",
       " 'All donations s',\n",
       " 'hould be made t',\n",
       " 'o \"Project Gute',\n",
       " 'nberg/IBC\", and',\n",
       " 'tax deductible ',\n",
       " 'to the extent a',\n",
       " 'llowable by law',\n",
       " ' (\"IBC\" is Illi',\n",
       " 'Benedictine Col',\n",
       " 'lege).  (Subscr',\n",
       " 'iptions to our ',\n",
       " 'paper newslette',\n",
       " 'For these and o',\n",
       " 'ther matters, p',\n",
       " 'Project Gutenbe',\n",
       " 'P. O. Box  2782',\n",
       " 'Champaign, IL 6',\n",
       " 'When all other ',\n",
       " 'email fails try',\n",
       " ' our Michael S.',\n",
       " ' Hart, Executiv',\n",
       " 'hart@vmd.cso.ui',\n",
       " 'uc.edu (interne',\n",
       " 't)   hart@uiucv',\n",
       " 'We would prefer',\n",
       " ' to send you th',\n",
       " 'is information ',\n",
       " '(Internet, Bitn',\n",
       " 'et, Compuserve,',\n",
       " ' ATTMAIL or MCI',\n",
       " 'If you have an ',\n",
       " 'FTP program (or',\n",
       " ' emulator), ple',\n",
       " 'FTP directly to',\n",
       " ' the Project Gu',\n",
       " 'tenberg archive',\n",
       " '[Mac users, do ',\n",
       " 'NOT point and c',\n",
       " 'ftp mrcnext.cso',\n",
       " 'login:  anonymo',\n",
       " 'password:  your',\n",
       " 'cd etext/etext9',\n",
       " 'or cd etext93 [',\n",
       " 'for new books] ',\n",
       " ' [now also in c',\n",
       " 'd etext/etext93',\n",
       " 'or cd etext/art',\n",
       " 'icles [get sugg',\n",
       " 'est gut for mor',\n",
       " 'dir [to see fil',\n",
       " 'get or mget [to',\n",
       " ' get files. . .',\n",
       " 'set bin for zip',\n",
       " 'for a list of b',\n",
       " 'GET NEW GUT for',\n",
       " ' general inform',\n",
       " 'MGET GUT* for n',\n",
       " '**Information p',\n",
       " 'repared by the ',\n",
       " 'Project Gutenbe',\n",
       " 'rg legal adviso',\n",
       " '***** SMALL PRI',\n",
       " 'NT! for COMPLET',\n",
       " 'E SHAKESPEARE *',\n",
       " 'THIS ELECTRONIC',\n",
       " ' VERSION OF THE',\n",
       " ' COMPLETE WORKS',\n",
       " 'SHAKESPEARE IS ',\n",
       " 'COPYRIGHT 1990-',\n",
       " '1993 BY WORLD L',\n",
       " 'AND IS PROVIDED',\n",
       " ' BY PROJECT GUT',\n",
       " 'ENBERG ETEXT OF',\n",
       " 'ILLINOIS BENEDI',\n",
       " 'CTINE COLLEGE W',\n",
       " 'ITH PERMISSION.',\n",
       " 'Since unlike ma',\n",
       " 'ny other Projec',\n",
       " 't Gutenberg-tm ',\n",
       " 'etexts, this et',\n",
       " 'is copyright pr',\n",
       " 'otected, and si',\n",
       " 'nce the materia',\n",
       " 'ls and methods ',\n",
       " 'use will effect',\n",
       " \" the Project's \",\n",
       " 'reputation, you',\n",
       " 'r right to copy',\n",
       " 'distribute it i',\n",
       " 's limited by th',\n",
       " 'e copyright and',\n",
       " ' other laws, an',\n",
       " 'the conditions ',\n",
       " 'of this \"Small ',\n",
       " 'Print!\" stateme',\n",
       " '  A) YOU MAY (A',\n",
       " 'ND ARE ENCOURAG',\n",
       " 'ED) TO DISTRIBU',\n",
       " 'TE ELECTRONIC A',\n",
       " 'MACHINE READABL',\n",
       " 'E COPIES OF THI',\n",
       " 'S ETEXT, SO LON',\n",
       " 'G AS SUCH COPIE',\n",
       " '(1) ARE FOR YOU',\n",
       " 'R OR OTHERS PER',\n",
       " 'SONAL USE ONLY,',\n",
       " ' AND (2) ARE NO',\n",
       " 'DISTRIBUTED OR ',\n",
       " 'USED COMMERCIAL',\n",
       " 'LY.  PROHIBITED',\n",
       " 'DISTRIBUTION IN',\n",
       " 'CLUDES BY ANY S',\n",
       " 'ERVICE THAT CHA',\n",
       " 'RGES FOR DOWNLO',\n",
       " 'TIME OR FOR MEM',\n",
       " '  B) This licen',\n",
       " 'se is subject t',\n",
       " 'o the condition',\n",
       " 's that you hono',\n",
       " 'the refund and ',\n",
       " 'replacement pro',\n",
       " 'visions of this',\n",
       " ' \"small print!\"',\n",
       " 'statement; and ',\n",
       " 'that you distri',\n",
       " 'bute exact copi',\n",
       " 'es of this etex',\n",
       " 'including this ',\n",
       " 'Small Print sta',\n",
       " 'tement.  Such c',\n",
       " 'compressed or a',\n",
       " 'ny proprietary ',\n",
       " 'form (including',\n",
       " ' any form resul',\n",
       " 'from word proce',\n",
       " 'ssing or hypert',\n",
       " 'ext software), ',\n",
       " '    (1) The ete',\n",
       " 'xt, when displa',\n",
       " 'yed, is clearly',\n",
       " ' readable, and ',\n",
       " '  *not* contain',\n",
       " ' characters oth',\n",
       " 'er than those i',\n",
       " '  author of the',\n",
       " ' work, although',\n",
       " ' tilde (~), ast',\n",
       " '  underline (_)',\n",
       " ' characters may',\n",
       " ' be used to con',\n",
       " 'vey punctuation',\n",
       " '  intended by t',\n",
       " 'he author, and ',\n",
       " 'additional char',\n",
       " 'acters may be u',\n",
       " '  to indicate h',\n",
       " 'ypertext links;',\n",
       " '    (2) The ete',\n",
       " 'xt is readily c',\n",
       " 'onvertible by t',\n",
       " 'he reader at no',\n",
       " '  expense into ',\n",
       " 'plain ASCII, EB',\n",
       " 'CDIC or equival',\n",
       " 'ent form by the',\n",
       " '  program that ',\n",
       " 'displays the et',\n",
       " 'ext (as is the ',\n",
       " 'case, for insta',\n",
       " '  with most wor',\n",
       " 'd processors); ',\n",
       " '    (3) You pro',\n",
       " 'vide or agree t',\n",
       " 'o provide on re',\n",
       " '  additional co',\n",
       " 'st, fee or expe',\n",
       " 'nse, a copy of ',\n",
       " 'the etext in pl',\n",
       " '2.  LIMITED WAR',\n",
       " 'RANTY; DISCLAIM',\n",
       " 'This etext may ',\n",
       " 'contain a \"Defe',\n",
       " 'ct\" in the form',\n",
       " ' of incomplete,',\n",
       " 'inaccurate or c',\n",
       " 'orrupt data, tr',\n",
       " 'anscription err',\n",
       " 'ors, a copyrigh',\n",
       " 'other infringem',\n",
       " 'ent, a defectiv',\n",
       " 'e or damaged di',\n",
       " 'sk, computer vi',\n",
       " 'or codes that d',\n",
       " 'amage or cannot',\n",
       " ' be read by you',\n",
       " 'r equipment.  B',\n",
       " 'for the \"Right ',\n",
       " 'of Replacement ',\n",
       " 'or Refund\" desc',\n",
       " 'ribed below, th',\n",
       " 'Project (and an',\n",
       " 'y other party y',\n",
       " 'ou may receive ',\n",
       " 'this etext from',\n",
       " 'a PROJECT GUTEN',\n",
       " 'BERG-tm etext) ',\n",
       " 'disclaims all l',\n",
       " 'iability to you',\n",
       " 'damages, costs ',\n",
       " 'and expenses, i',\n",
       " 'ncluding legal ',\n",
       " 'fees, and YOU H',\n",
       " 'NO REMEDIES FOR',\n",
       " ' NEGLIGENCE OR ',\n",
       " 'UNDER STRICT LI',\n",
       " 'ABILITY, OR FOR',\n",
       " 'BREACH OF WARRA',\n",
       " 'NTY OR CONTRACT',\n",
       " ', INCLUDING BUT',\n",
       " ' NOT LIMITED TO',\n",
       " 'INDIRECT, CONSE',\n",
       " 'QUENTIAL, PUNIT',\n",
       " 'IVE OR INCIDENT',\n",
       " 'AL DAMAGES, EVE',\n",
       " 'YOU GIVE NOTICE',\n",
       " ' OF THE POSSIBI',\n",
       " 'LITY OF SUCH DA',\n",
       " 'If you discover',\n",
       " ' a Defect in th',\n",
       " 'is etext within',\n",
       " ' 90 days of rec',\n",
       " 'ing it, you can',\n",
       " ' receive a refu',\n",
       " 'nd of the money',\n",
       " ' (if any) you p',\n",
       " 'for it by sendi',\n",
       " 'ng an explanato',\n",
       " 'ry note within ',\n",
       " 'that time to th',\n",
       " 'person you rece',\n",
       " 'ived it from.  ',\n",
       " 'If you received',\n",
       " ' it on a physic',\n",
       " 'medium, you mus',\n",
       " 't return it wit',\n",
       " 'h your note, an',\n",
       " 'd such person m',\n",
       " 'choose to alter',\n",
       " 'natively give y',\n",
       " 'ou a replacemen',\n",
       " 't copy.  If you',\n",
       " 'received it ele',\n",
       " 'ctronically, su',\n",
       " 'ch person may c',\n",
       " 'alternatively g',\n",
       " 'ive you a secon',\n",
       " 'd opportunity t',\n",
       " 'electronically.',\n",
       " 'THIS ETEXT IS O',\n",
       " 'THERWISE PROVID',\n",
       " 'ED TO YOU \"AS-I',\n",
       " 'WARRANTIES OF A',\n",
       " 'NY KIND, EXPRES',\n",
       " 'S OR IMPLIED, A',\n",
       " 'RE MADE TO YOU ',\n",
       " 'TO THE ETEXT OR',\n",
       " ' ANY MEDIUM IT ',\n",
       " 'MAY BE ON, INCL',\n",
       " 'LIMITED TO WARR',\n",
       " 'ANTIES OF MERCH',\n",
       " 'ANTABILITY OR F',\n",
       " 'PARTICULAR PURP',\n",
       " 'OSE.  Some stat',\n",
       " 'es do not allow',\n",
       " ' disclaimers of',\n",
       " 'implied warrant',\n",
       " 'ies or the excl',\n",
       " 'usion or limita',\n",
       " 'tion of consequ',\n",
       " 'tial damages, s',\n",
       " 'o the above dis',\n",
       " 'claimers and ex',\n",
       " 'clusions may no',\n",
       " 'apply to you, a',\n",
       " 'nd you may have',\n",
       " ' other legal ri',\n",
       " '3.  INDEMNITY: ',\n",
       " 'You will indemn',\n",
       " 'ify and hold th',\n",
       " 'directors, offi',\n",
       " 'cers, members a',\n",
       " 'nd agents harml',\n",
       " 'ess from all li',\n",
       " 'bility, cost an',\n",
       " 'd expense, incl',\n",
       " 'uding legal fee',\n",
       " 'directly or ind',\n",
       " 'irectly from an',\n",
       " 'y of the follow',\n",
       " 'ing that you do',\n",
       " 'cause: [A] dist',\n",
       " 'ribution of thi',\n",
       " 's etext, [B] al',\n",
       " 'modification, o',\n",
       " 'r addition to t',\n",
       " 'he etext, or [C',\n",
       " '4.  WHAT IF YOU',\n",
       " ' *WANT* TO SEND',\n",
       " ' MONEY EVEN IF ',\n",
       " \"YOU DON'T HAVE \",\n",
       " 'Project Gutenbe',\n",
       " 'rg is dedicated',\n",
       " ' to increasing ',\n",
       " 'public domain a',\n",
       " 'nd licensed wor',\n",
       " 'ks that can be ',\n",
       " 'freely distribu',\n",
       " 'in machine read',\n",
       " 'able form.  The',\n",
       " ' Project gratef',\n",
       " 'contributions i',\n",
       " 'n money, time, ',\n",
       " 'scanning machin',\n",
       " 'es, OCR softwar',\n",
       " 'public domain e',\n",
       " 'texts, royalty ',\n",
       " 'free copyright ',\n",
       " 'whatever else y',\n",
       " 'ou can think of',\n",
       " '.  Money should',\n",
       " ' be paid to \"Pr',\n",
       " 'ject Gutenberg ',\n",
       " 'Association / I',\n",
       " 'llinois Benedic',\n",
       " 'WRITE TO US! We',\n",
       " ' can be reached',\n",
       " '     Internet: ',\n",
       " 'hart@vmd.cso.ui',\n",
       " '       Bitnet: ',\n",
       " '   CompuServe: ',\n",
       " '>internet:hart@',\n",
       " '.vmd.cso.uiuc.e',\n",
       " '      Attmail: ',\n",
       " 'internet!vmd.cs',\n",
       " 'o.uiuc.edu!Hart',\n",
       " '        Mail:  ',\n",
       " 'Prof. Michael H',\n",
       " '               ',\n",
       " '               ',\n",
       " 'Champaign, IL 6',\n",
       " 'This \"Small Pri',\n",
       " 'nt!\" by Charles',\n",
       " ' B. Kramer, Att',\n",
       " 'Internet (72600',\n",
       " '.2026@compuserv',\n",
       " 'e.com); TEL: (2',\n",
       " '****   SMALL PR',\n",
       " 'INT! FOR __ COM',\n",
       " 'PLETE SHAKESPEA',\n",
       " '[\"Small Print\" ',\n",
       " '<<THIS ELECTRON',\n",
       " 'IC VERSION OF T',\n",
       " 'HE COMPLETE WOR',\n",
       " 'KS OF WILLIAM  ',\n",
       " 'SHAKESPEARE IS ',\n",
       " 'COPYRIGHT 1990-',\n",
       " '1993 BY WORLD L',\n",
       " 'IBRARY, INC., A',\n",
       " 'PROVIDED BY PRO',\n",
       " 'JECT GUTENBERG ',\n",
       " 'ETEXT OF ILLINO',\n",
       " 'IS BENEDICTINE ',\n",
       " 'WITH PERMISSION',\n",
       " '.  ELECTRONIC A',\n",
       " 'ND MACHINE READ',\n",
       " 'ABLE COPIES MAY',\n",
       " 'DISTRIBUTED SO ',\n",
       " 'LONG AS SUCH CO',\n",
       " 'PIES (1) ARE FO',\n",
       " 'R YOUR OR OTHER',\n",
       " 'PERSONAL USE ON',\n",
       " 'LY, AND (2) ARE',\n",
       " ' NOT DISTRIBUTE',\n",
       " 'D OR USED      ',\n",
       " 'COMMERCIALLY.  ',\n",
       " 'PROHIBITED COMM',\n",
       " 'ERCIAL DISTRIBU',\n",
       " 'TION INCLUDES B',\n",
       " 'SERVICE THAT CH',\n",
       " 'ARGES FOR DOWNL',\n",
       " 'OAD TIME OR FOR',\n",
       " ' MEMBERSHIP.>> ',\n",
       " 'by William Shak',\n",
       " '               ',\n",
       " '  From fairest ',\n",
       " 'creatures we de',\n",
       " '  That thereby ',\n",
       " \"beauty's rose m\",\n",
       " 'ight never die,',\n",
       " '  But as the ri',\n",
       " 'per should by t',\n",
       " '  His tender he',\n",
       " 'ir might bear h',\n",
       " '  But thou cont',\n",
       " 'racted to thine',\n",
       " ' own bright eye',\n",
       " \"  Feed'st thy l\",\n",
       " \"ight's flame wi\",\n",
       " 'th self-substan',\n",
       " '  Making a fami',\n",
       " 'ne where abunda',\n",
       " '  Thy self thy ',\n",
       " 'foe, to thy swe',\n",
       " 'et self too cru',\n",
       " '  Thou that art',\n",
       " \" now the world'\",\n",
       " 's fresh ornamen',\n",
       " '  And only hera',\n",
       " 'ld to the gaudy',\n",
       " '  Within thine ',\n",
       " 'own bud buriest',\n",
       " '  And tender ch',\n",
       " \"url mak'st wast\",\n",
       " 'e in niggarding',\n",
       " '    Pity the wo',\n",
       " 'rld, or else th',\n",
       " '    To eat the ',\n",
       " \"world's due, by\",\n",
       " ' the grave and ',\n",
       " '               ',\n",
       " '  When forty wi',\n",
       " 'nters shall bes',\n",
       " '  And dig deep ',\n",
       " 'trenches in thy',\n",
       " \" beauty's field\",\n",
       " \"  Thy youth's p\",\n",
       " 'roud livery so ',\n",
       " '  Will be a tat',\n",
       " 'tered weed of s',\n",
       " 'mall worth held',\n",
       " '  Then being as',\n",
       " 'ked, where all ',\n",
       " 'thy beauty lies',\n",
       " '  Where all the',\n",
       " ' treasure of th',\n",
       " '  To say within',\n",
       " ' thine own deep',\n",
       " '  Were an all-e',\n",
       " 'ating shame, an',\n",
       " 'd thriftless pr',\n",
       " '  How much more',\n",
       " ' praise deserve',\n",
       " \"d thy beauty's \",\n",
       " '  If thou could',\n",
       " \"st answer 'This\",\n",
       " ' fair child of ',\n",
       " '  Shall sum my ',\n",
       " 'count, and make',\n",
       " \" my old excuse'\",\n",
       " '  Proving his b',\n",
       " 'eauty by succes',\n",
       " '    This were t',\n",
       " 'o be new made w',\n",
       " 'hen thou art ol',\n",
       " '    And see thy',\n",
       " ' blood warm whe',\n",
       " \"n thou feel'st \",\n",
       " '               ',\n",
       " '  Look in thy g',\n",
       " 'lass and tell t',\n",
       " 'he face thou vi',\n",
       " '  Now is the ti',\n",
       " 'me that face sh',\n",
       " 'ould form anoth',\n",
       " '  Whose fresh r',\n",
       " 'epair if now th',\n",
       " 'ou not renewest',\n",
       " '  Thou dost beg',\n",
       " 'uile the world,',\n",
       " ' unbless some m',\n",
       " '  For where is ',\n",
       " 'she so fair who',\n",
       " 'se uneared womb',\n",
       " '  Disdains the ',\n",
       " 'tillage of thy ',\n",
       " '  Or who is he ',\n",
       " 'so fond will be',\n",
       " '  Of his self-l',\n",
       " 'ove to stop pos',\n",
       " '  Thou art thy ',\n",
       " \"mother's glass \",\n",
       " 'and she in thee',\n",
       " '  Calls back th',\n",
       " 'e lovely April ',\n",
       " '  So thou throu',\n",
       " 'gh windows of t',\n",
       " 'hine age shalt ',\n",
       " '  Despite of wr',\n",
       " 'inkles this thy',\n",
       " '    But if thou',\n",
       " ' live remembere',\n",
       " '    Die single ',\n",
       " 'and thine image',\n",
       " ' dies with thee',\n",
       " '               ',\n",
       " '  Unthrifty lov',\n",
       " 'eliness why dos',\n",
       " '  Upon thy self',\n",
       " \" thy beauty's l\",\n",
       " \"  Nature's bequ\",\n",
       " 'est gives nothi',\n",
       " 'ng but doth len',\n",
       " '  And being fra',\n",
       " 'nk she lends to',\n",
       " ' those are free',\n",
       " '  Then beauteou',\n",
       " 's niggard why d',\n",
       " 'ost thou abuse,',\n",
       " '  The bounteous',\n",
       " ' largess given ',\n",
       " '  Profitless us',\n",
       " 'urer why dost t',\n",
       " '  So great a su',\n",
       " 'm of sums yet c',\n",
       " '  For having tr',\n",
       " 'affic with thy ',\n",
       " '  Thou of thy s',\n",
       " 'elf thy sweet s',\n",
       " 'elf dost deceiv',\n",
       " '  Then how when',\n",
       " ' nature calls t',\n",
       " 'hee to be gone,',\n",
       " '  What acceptab',\n",
       " 'le audit canst ',\n",
       " '    Thy unused ',\n",
       " 'beauty must be ',\n",
       " 'tombed with the',\n",
       " '    Which used ',\n",
       " \"lives th' execu\",\n",
       " '               ',\n",
       " '  Those hours t',\n",
       " 'hat with gentle',\n",
       " ' work did frame',\n",
       " '  The lovely ga',\n",
       " 'ze where every ',\n",
       " '  Will play the',\n",
       " ' tyrants to the',\n",
       " '  And that unfa',\n",
       " 'ir which fairly',\n",
       " '  For never-res',\n",
       " 'ting time leads',\n",
       " '  To hideous wi',\n",
       " 'nter and confou',\n",
       " '  Sap checked w',\n",
       " 'ith frost and l',\n",
       " 'usty leaves qui',\n",
       " \"  Beauty o'er-s\",\n",
       " 'nowed and baren',\n",
       " 'ess every where',\n",
       " '  Then were not',\n",
       " \" summer's disti\",\n",
       " '  A liquid pris',\n",
       " 'oner pent in wa',\n",
       " \"  Beauty's effe\",\n",
       " 'ct with beauty ',\n",
       " '  Nor it nor no',\n",
       " ' remembrance wh',\n",
       " '    But flowers',\n",
       " ' distilled thou',\n",
       " 'gh they with wi',\n",
       " '    Leese but t',\n",
       " 'heir show, thei',\n",
       " 'r substance sti',\n",
       " 'll lives sweet.',\n",
       " '               ',\n",
       " '  Then let not ',\n",
       " \"winter's ragged\",\n",
       " '  In thee thy s',\n",
       " 'ummer ere thou ',\n",
       " '  Make sweet so',\n",
       " 'me vial; treasu',\n",
       " 're thou some pl',\n",
       " \"  With beauty's\",\n",
       " ' treasure ere i',\n",
       " 't be self-kille',\n",
       " '  That use is n',\n",
       " 'ot forbidden us',\n",
       " '  Which happies',\n",
       " ' those that pay',\n",
       " ' the willing lo',\n",
       " \"  That's for th\",\n",
       " 'y self to breed',\n",
       " '  Or ten times ',\n",
       " 'happier be it t',\n",
       " '  Ten times thy',\n",
       " ' self were happ',\n",
       " 'ier than thou a',\n",
       " '  If ten of thi',\n",
       " 'ne ten times re',\n",
       " '  Then what cou',\n",
       " 'ld death do if ',\n",
       " 'thou shouldst d',\n",
       " '  Leaving thee ',\n",
       " 'living in poste',\n",
       " '    Be not self',\n",
       " '-willed for tho',\n",
       " 'u art much too ',\n",
       " '    To be death',\n",
       " \"'s conquest and\",\n",
       " ' make worms thi',\n",
       " '               ',\n",
       " '  Lo in the ori',\n",
       " 'ent when the gr',\n",
       " '  Lifts up his ',\n",
       " 'burning head, e',\n",
       " '  Doth homage t',\n",
       " 'o his new-appea',\n",
       " '  Serving with ',\n",
       " 'looks his sacre',\n",
       " '  And having cl',\n",
       " 'imbed the steep',\n",
       " '-up heavenly hi',\n",
       " '  Resembling st',\n",
       " 'rong youth in h',\n",
       " '  Yet mortal lo',\n",
       " 'oks adore his b',\n",
       " '  Attending on ',\n",
       " 'his golden pilg',\n",
       " '  But when from',\n",
       " ' highmost pitch',\n",
       " ' with weary car',\n",
       " '  Like feeble a',\n",
       " 'ge he reeleth f',\n",
       " '  The eyes (for',\n",
       " 'e duteous) now ',\n",
       " '  From his low ',\n",
       " 'tract and look ',\n",
       " '    So thou, th',\n",
       " 'y self out-goin',\n",
       " '    Unlooked on',\n",
       " ' diest unless t',\n",
       " '               ',\n",
       " '  Music to hear',\n",
       " \", why hear'st t\",\n",
       " 'hou music sadly',\n",
       " '  Sweets with s',\n",
       " 'weets war not, ',\n",
       " 'joy delights in',\n",
       " \"  Why lov'st th\",\n",
       " 'ou that which t',\n",
       " \"hou receiv'st n\",\n",
       " '  Or else recei',\n",
       " \"v'st with pleas\",\n",
       " 'ure thine annoy',\n",
       " '  If the true c',\n",
       " 'oncord of well-',\n",
       " '  By unions mar',\n",
       " 'ried do offend ',\n",
       " '  They do but s',\n",
       " 'weetly chide th',\n",
       " 'ee, who confoun',\n",
       " '  In singleness',\n",
       " ' the parts that',\n",
       " ' thou shouldst ',\n",
       " '  Mark how one ',\n",
       " 'string sweet hu',\n",
       " 'sband to anothe',\n",
       " '  Strikes each ',\n",
       " 'in each by mutu',\n",
       " '  Resembling si',\n",
       " 're, and child, ',\n",
       " 'and happy mothe',\n",
       " '  Who all in on',\n",
       " 'e, one pleasing',\n",
       " '    Whose speec',\n",
       " 'hless song bein',\n",
       " 'g many, seeming',\n",
       " '    Sings this ',\n",
       " \"to thee, 'Thou \",\n",
       " 'single wilt pro',\n",
       " '               ',\n",
       " '  Is it for fea',\n",
       " 'r to wet a wido',\n",
       " '  That thou con',\n",
       " \"sum'st thy self\",\n",
       " ' in single life',\n",
       " '  Ah, if thou i',\n",
       " 'ssueless shalt ',\n",
       " '  The world wil',\n",
       " 'l wail thee lik',\n",
       " 'e a makeless wi',\n",
       " '  The world wil',\n",
       " 'l be thy widow ',\n",
       " 'and still weep,',\n",
       " '  That thou no ',\n",
       " 'form of thee ha',\n",
       " 'st left behind,',\n",
       " '  When every pr',\n",
       " 'ivate widow wel',\n",
       " \"  By children's\",\n",
       " ' eyes, her husb',\n",
       " \"and's shape in \",\n",
       " '  Look what an ',\n",
       " 'unthrift in the',\n",
       " ' world doth spe',\n",
       " '  Shifts but hi',\n",
       " 's place, for st',\n",
       " 'ill the world e',\n",
       " \"  But beauty's \",\n",
       " 'waste hath in t',\n",
       " 'he world an end',\n",
       " '  And kept unus',\n",
       " 'ed the user so ',\n",
       " '    No love tow',\n",
       " 'ard others in t',\n",
       " '    That on him',\n",
       " \"self such murd'\",\n",
       " 'rous shame comm',\n",
       " '               ',\n",
       " '  For shame den',\n",
       " 'y that thou bea',\n",
       " \"r'st love to an\",\n",
       " '  Who for thy s',\n",
       " 'elf art so unpr',\n",
       " '  Grant if thou',\n",
       " ' wilt, thou art',\n",
       " ' beloved of man',\n",
       " '  But that thou',\n",
       " \" none lov'st is\",\n",
       " '  For thou art ',\n",
       " 'so possessed wi',\n",
       " \"th murd'rous ha\",\n",
       " \"  That 'gainst \",\n",
       " 'thy self thou s',\n",
       " \"tick'st not to \",\n",
       " '  Seeking that ',\n",
       " 'beauteous roof ',\n",
       " '  Which to repa',\n",
       " 'ir should be th',\n",
       " 'y chief desire:',\n",
       " '  O change thy ',\n",
       " 'thought, that I',\n",
       " ' may change my ',\n",
       " '  Shall hate be',\n",
       " ' fairer lodged ',\n",
       " 'than gentle lov',\n",
       " '  Be as thy pre',\n",
       " 'sence is gracio',\n",
       " '  Or to thy sel',\n",
       " 'f at least kind',\n",
       " '-hearted prove,',\n",
       " '    Make thee a',\n",
       " 'nother self for',\n",
       " '    That beauty',\n",
       " ' still may live',\n",
       " ' in thine or th',\n",
       " '               ',\n",
       " '  As fast as th',\n",
       " 'ou shalt wane s',\n",
       " 'o fast thou gro',\n",
       " '  In one of thi',\n",
       " 'ne, from that w',\n",
       " 'hich thou depar',\n",
       " '  And that fres',\n",
       " 'h blood which y',\n",
       " 'oungly thou bes',\n",
       " '  Thou mayst ca',\n",
       " 'll thine, when ',\n",
       " 'thou from youth',\n",
       " '  Herein lives ',\n",
       " 'wisdom, beauty,',\n",
       " '  Without this ',\n",
       " 'folly, age, and',\n",
       " '  If all were m',\n",
       " 'inded so, the t',\n",
       " 'imes should cea',\n",
       " '  And threescor',\n",
       " 'e year would ma',\n",
       " 'ke the world aw',\n",
       " '  Let those who',\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = len(max(text, key=len))\n",
    "\n",
    "for i in range(len(text)):\n",
    "    while len(text[i]) < maxlen:\n",
    "        text[i] += ' '\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = []\n",
    "target_seqs = []\n",
    "\n",
    "for sentence in text:\n",
    "    input_seqs.append(sentence[:-1])\n",
    "    target_seqs.append(sentence[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This is the 10', 'his is the 100'),\n",
       " ('th Etext file ', 'h Etext file p'),\n",
       " ('resented by Pr', 'esented by Pro'),\n",
       " ('ject Gutenberg', 'ect Gutenberg,'),\n",
       " ('is presented i', 's presented in'),\n",
       " (' cooperation w', 'cooperation wi'),\n",
       " ('th World Libra', 'h World Librar'),\n",
       " ('y, Inc., from ', ', Inc., from t'),\n",
       " ('Library of the', 'ibrary of the '),\n",
       " ('Future and Sha', 'uture and Shak')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_set = []\n",
    "\n",
    "for sentence in text:\n",
    "    train_data_set.append((sentence[:-1],sentence[1:]))\n",
    "\n",
    "train_data_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([78, 72, 66, 48, 11, 66, 48, 11, 69, 72, 56, 11, 17, 47],\n",
       "  [72, 66, 48, 11, 66, 48, 11, 69, 72, 56, 11, 17, 47, 47]),\n",
       " ([69, 72, 11, 82, 69, 56, 54, 69, 11, 51, 66, 39, 56, 11],\n",
       "  [72, 11, 82, 69, 56, 54, 69, 11, 51, 66, 39, 56, 11, 28]),\n",
       " ([87, 56, 48, 56, 80, 69, 56, 10, 11, 29, 74, 11, 71, 87],\n",
       "  [56, 48, 56, 80, 69, 56, 10, 11, 29, 74, 11, 71, 87, 44]),\n",
       " ([37, 56, 33, 69, 11, 23, 62, 69, 56, 80, 29, 56, 87, 79],\n",
       "  [56, 33, 69, 11, 23, 62, 69, 56, 80, 29, 56, 87, 79, 85]),\n",
       " ([66, 48, 11, 28, 87, 56, 48, 56, 80, 69, 56, 10, 11, 66],\n",
       "  [48, 11, 28, 87, 56, 48, 56, 80, 69, 56, 10, 11, 66, 80]),\n",
       " ([11, 33, 44, 44, 28, 56, 87, 14, 69, 66, 44, 80, 11, 12],\n",
       "  [33, 44, 44, 28, 56, 87, 14, 69, 66, 44, 80, 11, 12, 66]),\n",
       " ([69, 72, 11, 20, 44, 87, 39, 10, 11, 30, 66, 29, 87, 14],\n",
       "  [72, 11, 20, 44, 87, 39, 10, 11, 30, 66, 29, 87, 14, 87]),\n",
       " ([74, 85, 11, 38, 80, 33, 3, 85, 11, 51, 87, 44, 68, 11],\n",
       "  [85, 11, 38, 80, 33, 3, 85, 11, 51, 87, 44, 68, 11, 69]),\n",
       " ([30, 66, 29, 87, 14, 87, 74, 11, 44, 51, 11, 69, 72, 56],\n",
       "  [66, 29, 87, 14, 87, 74, 11, 44, 51, 11, 69, 72, 56, 11]),\n",
       " ([9, 62, 69, 62, 87, 56, 11, 14, 80, 10, 11, 58, 72, 14],\n",
       "  [62, 69, 62, 87, 56, 11, 14, 80, 10, 11, 58, 72, 14, 83])]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(train_data_set)):\n",
    "    train_data_set[i] = ([char2int[char] for char in train_data_set[i][0]], [char2int[char] for char in train_data_set[i][1]])\n",
    "\n",
    "train_data_set[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set batch size when using real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303642"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "#batch_size = len(text)\n",
    "batch_size = 16\n",
    "\n",
    "def one_hot_encode(seq, dict_size, seq_len):\n",
    "    features = np.zeros((seq_len, dict_size), dtype=np.float32)\n",
    "    for i in range(seq_len):\n",
    "        features[i,seq[i]] = 1\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78, 72, 66, 48, 11, 66, 48, 11, 69, 72, 56, 11, 17, 47]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_set[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_encoded_train = [(one_hot_encode(train_data_set[i][0], dict_size, seq_len), train_data_set[i][1]) for i in range(len(train_data_set))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode target output\n",
    "# target_seqs_batched = []\n",
    "# batch_size\n",
    "\n",
    "# for index in target_seqs:\n",
    "#     batch_offset = index % batch_size\n",
    "#     if index - (index % batch_size) == 0:\n",
    "#         target_seqs_batched[]\n",
    "\n",
    "target_batched = torch.utils.data.DataLoader(hot_encoded_train[:2000], batch_size=batch_size, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(target_batched))\n",
    "\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): data must be a sequence (got DataLoader)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000007?line=0'>1</a>\u001b[0m input_final \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(input_seq_encoded)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000007?line=1'>2</a>\u001b[0m target_seq \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mTensor(target_batched)\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): data must be a sequence (got DataLoader)"
     ]
    }
   ],
   "source": [
    "input_final = torch.from_numpy(input_seq_encoded)\n",
    "target_seq = torch.Tensor(target_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([303642, 14])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda:\n",
    "    device = torch.device('gpu')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=100, n_layers=10)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs = 100\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100............. Loss: 3.3464\n",
      "Epoch: 20/100............. Loss: 3.2245\n",
      "Epoch: 30/100............. Loss: 3.2022\n",
      "Epoch: 40/100............. Loss: 3.3893\n",
      "Epoch: 50/100............. Loss: 3.1892\n",
      "Epoch: 60/100............. Loss: 3.3161\n",
      "Epoch: 70/100............. Loss: 3.3968\n",
      "Epoch: 80/100............. Loss: 3.4370\n",
      "Epoch: 90/100............. Loss: 3.0402\n",
      "Epoch: 100/100............. Loss: 3.4283\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "input_seq = input_final.to(device)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    batch_index = 0\n",
    "    for data in target_batched:\n",
    "        batch_index += 1\n",
    "        x , y = data\n",
    "\n",
    "        x = torch.Tensor(x)\n",
    "        y = torch.stack(y)\n",
    "\n",
    "        input_seq = x.to(device)\n",
    "        target_seq = y.to(device)\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "\n",
    "        output, hidden = model(input_seq)\n",
    "        output = output.to(device)\n",
    "\n",
    "        loss = criterion(output, target_seq.view(-1).long())\n",
    "        loss.backward() # Does backpropagation and calculates gradients\n",
    "        optimizer.step() # Updates the weights accordingly\n",
    "        \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, character):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([char2int[c] for c in character])\n",
    "    print(\"dict_size \", dict_size)\n",
    "    character = one_hot_encode(character, dict_size, character.shape[0])\n",
    "    print(\"character: \", character)\n",
    "    character = torch.from_numpy(character)\n",
    "    character = character.to(device)\n",
    "    \n",
    "    out, hidden = model(character)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    # Taking the class with the highest probability score from the output\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return int2char[char_ind], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, out_len, start='hey'):\n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    print(\"chars\", chars)\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars ['i', ' ', 'l', 'o', 'v', 'e']\n",
      "dict_size  88\n",
      "seq:   [66 11 39 44 24 56]\n",
      "i, seq[i]  0 66\n",
      "i, seq[i]  1 11\n",
      "i, seq[i]  2 39\n",
      "i, seq[i]  3 44\n",
      "i, seq[i]  4 24\n",
      "i, seq[i]  5 56\n",
      "character:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx should also be 2-D but got 3-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000015?line=0'>1</a>\u001b[0m sample(model, \u001b[39m15\u001b[39;49m, start\u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mi love\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb Cell 25\u001b[0m in \u001b[0;36msample\u001b[0;34m(model, out_len, start)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000015?line=7'>8</a>\u001b[0m \u001b[39m# Now pass in the previous characters and get a new one\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000015?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m ii \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(size):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000015?line=9'>10</a>\u001b[0m     char, h \u001b[39m=\u001b[39m predict(model, chars)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000015?line=10'>11</a>\u001b[0m     chars\u001b[39m.\u001b[39mappend(char)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000015?line=12'>13</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(chars)\n",
      "\u001b[1;32m/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb Cell 25\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, character)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000015?line=6'>7</a>\u001b[0m character \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(character)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000015?line=7'>8</a>\u001b[0m character \u001b[39m=\u001b[39m character\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000015?line=9'>10</a>\u001b[0m out, hidden \u001b[39m=\u001b[39m model(character)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000015?line=11'>12</a>\u001b[0m prob \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(out[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mdata\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000015?line=12'>13</a>\u001b[0m \u001b[39m# Taking the class with the highest probability score from the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb Cell 25\u001b[0m in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000015?line=19'>20</a>\u001b[0m hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_hidden(batch_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000015?line=21'>22</a>\u001b[0m \u001b[39m# Passing in the input and hidden state into the model and obtaining outputs\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000015?line=22'>23</a>\u001b[0m out, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x, hidden)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000015?line=24'>25</a>\u001b[0m \u001b[39m# Reshaping the outputs such that it can be fit into the fully connected layer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roman/workspace/dl-experiments/char-rnn/char-rnn.ipynb#ch0000015?line=25'>26</a>\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_dim)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:445\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[39mif\u001b[39;00m hx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    444\u001b[0m         \u001b[39mif\u001b[39;00m hx\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> 445\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    446\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFor unbatched 2-D input, hx should also be 2-D but got \u001b[39m\u001b[39m{\u001b[39;00mhx\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D tensor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    447\u001b[0m         hx \u001b[39m=\u001b[39m hx\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m    448\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx should also be 2-D but got 3-D tensor"
     ]
    }
   ],
   "source": [
    "sample(model, 15, start= 'i love')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
